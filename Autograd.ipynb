{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autograd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTJFZX3Lve5BjhiIilGzsH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seulayoon/PyTorch-Tutorial/blob/main/Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF0U3BuDLK3r"
      },
      "source": [
        "# AUTOGRAD: 자동 미분\n",
        "\n",
        "autograd 패키지는 Tensor의 모든 연산에 대해 자동 미분을 제공.\n",
        "실행-기반-정의(define-by-run) 프레임워크로, 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 뜻이며, 역전파는 학습 과정의 매 단계마다 달라진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzfWhI8hLqsA"
      },
      "source": [
        "## Tensor\n",
        "\n",
        "패키지의 중심에는 torch.Tensor 클래스가 있다. 만약 .requires_grad 속성을 True로  설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)하기 시작한다. 계산이 완료된 후 .backward()를 호출하여 모든 변화도(gradient)를 자동으로 계산할 수 있다. 이 Tensor의 변화도는 .grad 속성에 누적된다.\n",
        "\n",
        "Tensor가 기록을 추적하는 것을 중단하게 하려면, .detach()를 호출하여 연산 기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지할 수 있다.\n",
        "\n",
        "기록을 추적하는 것(과 메모리를 사용하는 것)을 방지하기 위해, 코드 블럭을 withtorch.no_grad(): 로 감쌀 수 있다. 이는 특히 변화도(gradient)는 필요없지만, requries_grad=True가 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evaluate)할 때 유용하다.\n",
        "\n",
        "Autograd 구현에서 매우 중요한 클래스는 Function 클래스이다.\n",
        "Tensor와 Function은 서로 연결되어 있으며, 모든 연산 과정을 부호화(encode)하여 순환하지 않는 그래프(acyclic graph)를 생성한다. 각 tensor는 .grad_fn 속성을 갖고 있는데, 이는 Tensor를 생성한 Function을 참조하고 있다. (단, 사용자가 만든 Tensor는 예외로로, 이 때 grad_fn은 None입니다.)\n",
        "\n",
        "도함수를 계산하기 위해서는 Tensor의 .backward()를 호출하면 된다. 만약 Tensor가 스칼라인 경우(예. 하나의 요소소 값만 갖는 등)에는 backward에 인자를 정해줄 필요가 없다. 하지만 여러 개의 요소를 갖고 있을 때는 tensor의 모양을 gradient의 인자로 지정할 필요가 있다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aab19Io1LCaK"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dFWF9CjZ5Q8",
        "outputId": "d1eb0cb8-cf99-4783-c336-0bca02171033"
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True) # tensor 생성하고 연산을 기록한다.\n",
        "print(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a6Zbm0UaLTT",
        "outputId": "991c6677-1050-4960-c154-fddbfed83eca"
      },
      "source": [
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EmUbGVhaQ6g",
        "outputId": "ae1d2845-a9f6-4c90-ddf0-b9f20057aec0"
      },
      "source": [
        "print(y.grad_fn)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<AddBackward0 object at 0x7f06736390d0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weMizquqaUhZ",
        "outputId": "22fa8785-60df-425f-faf3-2634d6ec23ca"
      },
      "source": [
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(z, out)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JStB1l58a0wl"
      },
      "source": [
        ".requires_grad_( ... ) 는 기존 Tensor의 requires_grad 값을 바꿔치기(in-place)하여 변경한다. 입력값이 지정되지 않으면 기본값은 False이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7wsgThAajSp",
        "outputId": "a2003514-aace-497b-ce95-e71fb244a7ec"
      },
      "source": [
        "a = torch.randn(2, 2)\n",
        "a = ((a * 3) / (a - 1))\n",
        "print(a.requires_grad)\n",
        "\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "\n",
        "b = (a * a).sum()\n",
        "print(b.grad_fn)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "<SumBackward0 object at 0x7f0673641ed0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFwGxuUQEsPa"
      },
      "source": [
        "# 변화도(Gradient)\n",
        "\n",
        "이제 역전파(backprop)를 해보자. out은 하나의 스칼라 값만 가지고 있기 때문에, out.backward()는 out.backward(torch.tensor(1.))과 동일하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_qb7EMVbbzt"
      },
      "source": [
        "out.backward()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wghbg0JAFBAR",
        "outputId": "ca9e4e37-24c9-4f0f-86b4-a03df46dc767"
      },
      "source": [
        "print(x.grad) # 변화도 d(out)/dx 출력"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LExh8LhPG-Wr"
      },
      "source": [
        "일반적으로, torch.autograd는 벡터-야코비안(Jacobian) 곱을 계산하는 엔진"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uho7j_byFRgE",
        "outputId": "187baf69-5a59-4894-cc59-5a07ebd82dde"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "  y = y * 2\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 734.3218, -575.5486, -785.3188], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRFw4pUTH8EX"
      },
      "source": [
        "이 경우 y는 더 이상 스칼라 값이 아니다. torch.autograd는 전체 야코비안을 직접 계산할 수는 없지만, 벡터-야코비안 곱은 간단히 backward에 해당 벡터를 인자로 제공하여 얻을 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVIkHqTqHs6p",
        "outputId": "c5b3ee62-1f38-41d3-f872-8223a62bccf3"
      },
      "source": [
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
        "y.backward(v)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VskKz36cFzi",
        "outputId": "0ba32615-b3a9-44a1-f490-e5bea80cc829"
      },
      "source": [
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad(): # autograd가 Tensor들의 연산 기록을 추적하는 것을 멈춤\n",
        "  print((x ** 2).requires_grad)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRfYvMbwcrvb"
      },
      "source": [
        ".detach()를 호출하여 내용물(content)은 같지만 require_grad가 다른 새로운 Tensor를 가져온다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXetmqIuck4q",
        "outputId": "c0634951-3eba-4da5-d0b6-adea08483dda"
      },
      "source": [
        "print(x.requires_grad)\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "print(x.eq(y).all())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}